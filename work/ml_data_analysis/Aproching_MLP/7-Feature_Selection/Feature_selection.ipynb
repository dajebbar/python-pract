{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Feature_selection.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Remove features with very low variance\n",
        "\n",
        "If the features have a very low variance (i.e. very close to 0), they are close to being constant and thus, do not add any value to any model at all. It would just be nice to get rid of them and hence lower the complexity.\n",
        "\n",
        "```\n",
        "from sklearn.feature_selection import VarianceThreshold\n",
        "data = ...\n",
        "var_thresh = VarianceThreshold(threshold=0.1)\n",
        "transformed_data = var_thresh.fit_transform(data)\n",
        "# transformed data will have all columns with variance less\n",
        "# than 0.1 removed\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "nZ0VGI_OxpbX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Remove features which have a high correlation\n",
        "\n",
        "For calculating the correlation between different numerical features, you can use the **Pearson correlation**.\n"
      ],
      "metadata": {
        "id": "sQeKSxgGyHFx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 332
        },
        "id": "zo6OsXY-w_cV",
        "outputId": "258673f2-9899-4cd2-96b1-0a5f3aeb540f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "               MedInc  HouseAge  AveRooms  AveBedrms  Population  AveOccup  \\\n",
              "MedInc       1.000000 -0.119034  0.326895  -0.062040    0.004834  0.018766   \n",
              "HouseAge    -0.119034  1.000000 -0.153277  -0.077747   -0.296244  0.013191   \n",
              "AveRooms     0.326895 -0.153277  1.000000   0.847621   -0.072213 -0.004852   \n",
              "AveBedrms   -0.062040 -0.077747  0.847621   1.000000   -0.066197 -0.006181   \n",
              "Population   0.004834 -0.296244 -0.072213  -0.066197    1.000000  0.069863   \n",
              "AveOccup     0.018766  0.013191 -0.004852  -0.006181    0.069863  1.000000   \n",
              "Latitude    -0.079809  0.011173  0.106389   0.069721   -0.108785  0.002366   \n",
              "Longitude   -0.015176 -0.108197 -0.027540   0.013344    0.099773  0.002476   \n",
              "MedInc_Sqrt  0.984329 -0.132797  0.326688  -0.066910    0.018415  0.015266   \n",
              "\n",
              "             Latitude  Longitude  MedInc_Sqrt  \n",
              "MedInc      -0.079809  -0.015176     0.984329  \n",
              "HouseAge     0.011173  -0.108197    -0.132797  \n",
              "AveRooms     0.106389  -0.027540     0.326688  \n",
              "AveBedrms    0.069721   0.013344    -0.066910  \n",
              "Population  -0.108785   0.099773     0.018415  \n",
              "AveOccup     0.002366   0.002476     0.015266  \n",
              "Latitude     1.000000  -0.924664    -0.084303  \n",
              "Longitude   -0.924664   1.000000    -0.015569  \n",
              "MedInc_Sqrt -0.084303  -0.015569     1.000000  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-154ab2b5-5aa0-4ebb-8df0-5531bec7c7d7\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>MedInc</th>\n",
              "      <th>HouseAge</th>\n",
              "      <th>AveRooms</th>\n",
              "      <th>AveBedrms</th>\n",
              "      <th>Population</th>\n",
              "      <th>AveOccup</th>\n",
              "      <th>Latitude</th>\n",
              "      <th>Longitude</th>\n",
              "      <th>MedInc_Sqrt</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>MedInc</th>\n",
              "      <td>1.000000</td>\n",
              "      <td>-0.119034</td>\n",
              "      <td>0.326895</td>\n",
              "      <td>-0.062040</td>\n",
              "      <td>0.004834</td>\n",
              "      <td>0.018766</td>\n",
              "      <td>-0.079809</td>\n",
              "      <td>-0.015176</td>\n",
              "      <td>0.984329</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>HouseAge</th>\n",
              "      <td>-0.119034</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>-0.153277</td>\n",
              "      <td>-0.077747</td>\n",
              "      <td>-0.296244</td>\n",
              "      <td>0.013191</td>\n",
              "      <td>0.011173</td>\n",
              "      <td>-0.108197</td>\n",
              "      <td>-0.132797</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>AveRooms</th>\n",
              "      <td>0.326895</td>\n",
              "      <td>-0.153277</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.847621</td>\n",
              "      <td>-0.072213</td>\n",
              "      <td>-0.004852</td>\n",
              "      <td>0.106389</td>\n",
              "      <td>-0.027540</td>\n",
              "      <td>0.326688</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>AveBedrms</th>\n",
              "      <td>-0.062040</td>\n",
              "      <td>-0.077747</td>\n",
              "      <td>0.847621</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>-0.066197</td>\n",
              "      <td>-0.006181</td>\n",
              "      <td>0.069721</td>\n",
              "      <td>0.013344</td>\n",
              "      <td>-0.066910</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Population</th>\n",
              "      <td>0.004834</td>\n",
              "      <td>-0.296244</td>\n",
              "      <td>-0.072213</td>\n",
              "      <td>-0.066197</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.069863</td>\n",
              "      <td>-0.108785</td>\n",
              "      <td>0.099773</td>\n",
              "      <td>0.018415</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>AveOccup</th>\n",
              "      <td>0.018766</td>\n",
              "      <td>0.013191</td>\n",
              "      <td>-0.004852</td>\n",
              "      <td>-0.006181</td>\n",
              "      <td>0.069863</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.002366</td>\n",
              "      <td>0.002476</td>\n",
              "      <td>0.015266</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Latitude</th>\n",
              "      <td>-0.079809</td>\n",
              "      <td>0.011173</td>\n",
              "      <td>0.106389</td>\n",
              "      <td>0.069721</td>\n",
              "      <td>-0.108785</td>\n",
              "      <td>0.002366</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>-0.924664</td>\n",
              "      <td>-0.084303</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Longitude</th>\n",
              "      <td>-0.015176</td>\n",
              "      <td>-0.108197</td>\n",
              "      <td>-0.027540</td>\n",
              "      <td>0.013344</td>\n",
              "      <td>0.099773</td>\n",
              "      <td>0.002476</td>\n",
              "      <td>-0.924664</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>-0.015569</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>MedInc_Sqrt</th>\n",
              "      <td>0.984329</td>\n",
              "      <td>-0.132797</td>\n",
              "      <td>0.326688</td>\n",
              "      <td>-0.066910</td>\n",
              "      <td>0.018415</td>\n",
              "      <td>0.015266</td>\n",
              "      <td>-0.084303</td>\n",
              "      <td>-0.015569</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-154ab2b5-5aa0-4ebb-8df0-5531bec7c7d7')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-154ab2b5-5aa0-4ebb-8df0-5531bec7c7d7 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-154ab2b5-5aa0-4ebb-8df0-5531bec7c7d7');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "\n",
        "# fetch a regression dataset\n",
        "data = fetch_california_housing()\n",
        "X, col_names, y = data.data, data['feature_names'], data.target\n",
        "\n",
        "# convert to pandas dataframe\n",
        "df = pd.DataFrame(X, columns=col_names)\n",
        "\n",
        "# introduce a highly correlated column\n",
        "df.loc[:, \"MedInc_Sqrt\"] = df.MedInc.apply(np.sqrt)\n",
        "\n",
        "# get correlation matrix (pearson)\n",
        "df.corr()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We see that the feature *MedInc_Sqrt* has a very high correlation with *MedInc*. We can thus remove one of them. "
      ],
      "metadata": {
        "id": "tCST26ltzrrj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Univariate ways of feature selection\n"
      ],
      "metadata": {
        "id": "RuqysWSpz1yv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Univariate feature selection** is nothing but a scoring of each feature against a given target.\n",
        "**Mutual information, ANOVA F-test** and **chi2** are some of the most popular methods for univariate feature selection. There are two ways of using these in scikitlearn.\n",
        "- `SelectKBest`: It keeps the top-k scoring features\n",
        "- `SelectPercentile`: It keeps the top features which are in a percentage\n",
        "specified by the user  \n",
        "\n",
        "It must be noted that you can use **chi2** only for data which is non negative in nature. This is a particularly useful feature selection technique in natural language processing when we have a bag of words or tf-idf based features. It’s best to create a wrapper for univariate feature selection that you can use for almost any new problem."
      ],
      "metadata": {
        "id": "Ehd3tATU0KFO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_selection import chi2\n",
        "from sklearn.feature_selection import f_classif\n",
        "from sklearn.feature_selection import f_regression\n",
        "from sklearn.feature_selection import mutual_info_classif\n",
        "from sklearn.feature_selection import mutual_info_regression\n",
        "from sklearn.feature_selection import SelectKBest\n",
        "from sklearn.feature_selection import SelectPercentile\n",
        "\n",
        "class UnivariateFeatureSelection:\n",
        "  def __init__(self, n_features, problem_type, scoring):\n",
        "      '''\n",
        "      Custom univariate feature selection wrapper on\n",
        "      different univariate feature selection models from\n",
        "      scikit-learn.\n",
        "      :param n_features: SelectPercentile if float else SelectKBest\n",
        "      :param problem_type: classification or regression\n",
        "      :param scoring: scoring function, string\n",
        "      '''\n",
        "      # for a given problem type, there are only\n",
        "      # a few valid scoring methods\n",
        "      # you can extend this with your own custom\n",
        "      # methods if you wish\n",
        "      if problem_type == 'classification':\n",
        "        valid_scoring = {\n",
        "            'f_classif': f_classif,\n",
        "            'mutual_info_classif': mutual_info_classif,\n",
        "            'chi2': chi2,\n",
        "        }\n",
        "      \n",
        "      else:\n",
        "        valid_scoring = {\n",
        "            'f_regression': f_regression,\n",
        "            'mutual_info_regression': mutual_info_regression,\n",
        "        }\n",
        "      \n",
        "      # raise exception if we do not have a valid scoring method\n",
        "      if scoring not in valid_scoring:\n",
        "        raise Exception('Invalid Scoring function')\n",
        "      \n",
        "      # if n_features is int, we use selectkbest\n",
        "      # if n_features is float, we use selectpercentile\n",
        "      # please note that it is int in both cases in sklearn\n",
        "      if isinstance(n_features, int):\n",
        "        self.selection = SelectKBest(valid_scoring[scoring], k=n_features)\n",
        "      \n",
        "      elif isinstance(n_features, float):\n",
        "        self.selection = SelectPercentile(valid_scoring[scoring],\n",
        "                                percentile= int(n_features * 100),\n",
        "                    )\n",
        "      else:\n",
        "        raise Exception('Invalid type of feature')\n",
        "    \n",
        "  # same fit function\n",
        "  def fit(self, X, y):\n",
        "    return self.selection.fit(X, y)\n",
        "    \n",
        "  # same transform\n",
        "  def transform(self, X):\n",
        "    return self.selection.transform(X)\n",
        "  \n",
        "  # same fit_transform\n",
        "  def fit_transform(self, X, y):\n",
        "    return self.selection.fit_transform(X, y)\n"
      ],
      "metadata": {
        "id": "75vd0ke0zSTV"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using this class is pretty simple\n",
        "\n",
        "\n",
        "```\n",
        "ufs = UnivariateFeatureSelction(\n",
        "        n_features=0.1,\n",
        "        problem_type=\"regression\",\n",
        "        scoring=\"f_regression\"\n",
        ")\n",
        "\n",
        "ufs.fit(X, y)\n",
        "\n",
        "X_transformed = ufs.transform(X)\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "1Fl9CgkW5P4z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Univariate feature selection may not always perform well. Most of the time, people prefer doing feature selection using a machine learning model. Let’s see how that is done.  \n",
        "The simplest form of feature selection that uses a model for selection is known as **greedy feature selection**. In greedy feature selection, the **first step** is to choose a model. **The second step** is to select a loss/scoring function. And the **third** and final step is to iteratively evaluate each feature and add it to the list of “good” features if it improves loss/score.  \n",
        "\n",
        "Let’s see how it works by looking at how its implemented"
      ],
      "metadata": {
        "id": "v63y9Oe56OKZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from sklearn import linear_model\n",
        "from sklearn import metrics\n",
        "from sklearn.datasets import make_classification\n",
        "\n",
        "\n",
        "class GreedyFeatureSelection:\n",
        "  '''\n",
        "  A simple and custom class for greedy feature selection.\n",
        "  You will need to modify it quite a bit to make it suitable\n",
        "  for your dataset.\n",
        "  '''\n",
        "  def evaluate_score(self, X, y):\n",
        "   '''\n",
        "   This function evaluates model on data and returns\n",
        "   Area Under ROC Curve (AUC)\n",
        "   NOTE: We fit the data and calculate AUC on same data.\n",
        "   WE ARE OVERFITTING HERE.\n",
        "   But this is also a way to achieve greedy selection.\n",
        "   k-fold will take k times longer.\n",
        "   If you want to implement it in really correct way,\n",
        "   calculate OOF AUC and return mean AUC over k folds.\n",
        "   :param X: training data\n",
        "   :param y: targets\n",
        "   :return: overfitted area under the roc curve\n",
        "   '''\n",
        "   # fit the logistic regression model,\n",
        "   # and calculate AUC on same data\n",
        "   # again: BEWARE\n",
        "   # you can choose any model that suits your data\n",
        "   model = linear_model.LogisticRegression()\n",
        "   model.fit(X, y)\n",
        "   predictions = model.predict_proba(X)[:, 1]\n",
        "   auc = metrics.roc_auc_score(y, predictions)\n",
        "   \n",
        "   return auc\n",
        "\n",
        "  def _feature_selection(self, X, y):\n",
        "    '''\n",
        "    This function does the actual greedy selection\n",
        "    :param X: data, numpy array\n",
        "    :param y: targets, numpy array\n",
        "    :return: (best scores, best features)\n",
        "    '''\n",
        "\n",
        "    # initialize good features list\n",
        "    # and best scores to keep track of both\n",
        "    good_features = []\n",
        "    best_scores = []\n",
        "\n",
        "    # calculate the number of features\n",
        "    num_features = X.shape[1]\n",
        "\n",
        "    # infinite loop\n",
        "    while True:\n",
        "      # initialize best feature and score of this loop\n",
        "      this_feature = None\n",
        "      best_score = 0\n",
        "\n",
        "      # loop over all features\n",
        "      for feature in range(num_features):\n",
        "        # if feature is already in good features,\n",
        "        # skip this for loop\n",
        "        if feature in good_features:\n",
        "          continue\n",
        "\n",
        "        # selected features are all good features till now\n",
        "        # and current feature\n",
        "        selected_features = good_features + [feature]\n",
        "\n",
        "        # remove all other features from data\n",
        "        xtrain = X[:, selected_features]\n",
        "\n",
        "        # calculate the score, in our case, AUC\n",
        "        score = self.evaluate_score(xtrain, y)\n",
        "\n",
        "        # if score is greater than the best score\n",
        "        # of this loop, change best score and best feature\n",
        "        if score > best_score:\n",
        "          this_feature = feature\n",
        "          best_score = score\n",
        "      \n",
        "      # if we have selected a feature, add it\n",
        "      # to the good feature list and update best scores list\n",
        "      if this_feature != None:\n",
        "        good_features.append(this_feature)\n",
        "        best_scores.append(best_score)\n",
        "      \n",
        "      # if we didnt improve during the last two rounds,\n",
        "      # exit the while loop\n",
        "      if len(best_scores) > 2:\n",
        "        if best_scores[-1] < best_scores[-2]:\n",
        "          break\n",
        "      \n",
        "    # return best scores and good features\n",
        "    return best_scores[:-1], good_features[:-1]\n",
        "\n",
        "  def __call__(self, X, y):\n",
        "    '''\n",
        "    Call function will call the class on a set of arguments\n",
        "    '''\n",
        "    # select features, return scores and selected indices\n",
        "    scores, features = self._feature_selection(X, y)\n",
        "\n",
        "    # transform data with selected features\n",
        "    return X[:, features], scores"
      ],
      "metadata": {
        "id": "L3Eu86gf5Luk"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# generate binary classification data\n",
        "X, y = make_classification(n_samples=1000, n_features=100)\n",
        "\n",
        "# transform data by greedy feature selection\n",
        "X_transformed, scores = GreedyFeatureSelection()(X, y)"
      ],
      "metadata": {
        "id": "jFkCTl0xAzR0"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_transformed"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RVPcySWPA8rw",
        "outputId": "d62f4e4d-fec5-4ca0-e804-004315088c99"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 2.49465684,  0.45834549, -0.654047  , ...,  1.45420819,\n",
              "        -1.31165788, -0.93561029],\n",
              "       [ 1.63012884,  0.1387105 ,  0.14786767, ...,  0.17249789,\n",
              "         0.60340295, -0.03461217],\n",
              "       [ 0.34413772, -1.49736071,  1.13457406, ..., -0.99008602,\n",
              "        -1.66534354,  0.71360958],\n",
              "       ...,\n",
              "       [-1.37227136, -2.3429267 , -1.61080285, ...,  0.00478079,\n",
              "         0.82940437, -0.39292466],\n",
              "       [-0.78435632,  1.01822594,  0.31546725, ..., -0.2391723 ,\n",
              "        -0.43753159,  0.19486585],\n",
              "       [-0.20977253,  0.09279691,  0.10827515, ..., -2.15896469,\n",
              "        -0.33161366, -0.3331654 ]])"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "scores"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tM-JXA7mBUnS",
        "outputId": "220e86be-c724-4cd1-f007-a1d5420f0aa9"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.9645279999999999,\n",
              " 0.9666760000000001,\n",
              " 0.967468,\n",
              " 0.9682399999999999,\n",
              " 0.968796,\n",
              " 0.96938,\n",
              " 0.969996,\n",
              " 0.9705439999999999,\n",
              " 0.971008,\n",
              " 0.971616,\n",
              " 0.972036,\n",
              " 0.97238,\n",
              " 0.9727239999999999,\n",
              " 0.973,\n",
              " 0.973316,\n",
              " 0.9736199999999999,\n",
              " 0.9739199999999999,\n",
              " 0.9740279999999999,\n",
              " 0.9741120000000001,\n",
              " 0.974352,\n",
              " 0.974532,\n",
              " 0.97478,\n",
              " 0.9749800000000001,\n",
              " 0.975104,\n",
              " 0.975344,\n",
              " 0.975492,\n",
              " 0.975776,\n",
              " 0.97596,\n",
              " 0.9761839999999999,\n",
              " 0.976308,\n",
              " 0.976388,\n",
              " 0.9766279999999999,\n",
              " 0.976796,\n",
              " 0.9769760000000001,\n",
              " 0.977152,\n",
              " 0.9773839999999999,\n",
              " 0.977476,\n",
              " 0.9775,\n",
              " 0.977536,\n",
              " 0.9776119999999999,\n",
              " 0.977656,\n",
              " 0.977768,\n",
              " 0.977892,\n",
              " 0.977996,\n",
              " 0.9780279999999999,\n",
              " 0.978076,\n",
              " 0.9780880000000001,\n",
              " 0.978092]"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Recursive Feature Elimination (RFE)\n",
        "\n",
        "In the previous method, we started with one feature and kept adding new features, but in RFE, we start with all features and keep removing one feature in every iteration that provides the least value to a given model. But how to do we know which feature offers the least value? Well, if we use models like linear support vector machine (SVM) or logistic regression, we get a coefficient for each feature which decides the importance of the features. In case of any tree-based models, we get feature importance in place of coefficients. In each iteration, we can eliminate the least important feature and keep eliminating it until we reach the number of features needed. So, yes, we have the ability to decide how many features we want to keep."
      ],
      "metadata": {
        "id": "Rwy6lNttFwvZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "2KtkyLgABXqN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}